{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an already pretrained SBERT to compute cosine similarities which would then be used \n",
    "\n",
    "df=pd.read_csv(\"stsb_train.csv\")\n",
    "df=df.drop(\"idx\",axis=1)\n",
    "df_cosine_labels = df.copy() \n",
    "df_cosine_labels[\"label\"] = df_cosine_labels[\"label\"] /df_cosine_labels[\"label\"].abs().max() \n",
    "\n",
    "\n",
    "df=pd.read_csv(\"stsb_validation.csv\")\n",
    "df=df.drop(\"idx\",axis=1)\n",
    "validation_cosine_labels = df.copy() \n",
    "validation_cosine_labels[\"label\"] = validation_cosine_labels[\"label\"] /validation_cosine_labels[\"label\"].abs().max() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "for i,row in df_cosine_labels.iterrows():\n",
    "    train_examples.append(InputExample(texts=[row[\"sentence1\"], row[\"sentence2\"]],label=row[\"label\"]))\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "val_examples = []\n",
    "for i,row in validation_cosine_labels.iterrows():\n",
    "    val_examples.append(InputExample(texts=[row[\"sentence1\"], row[\"sentence2\"]],label=row[\"label\"]))\n",
    "\n",
    "\n",
    "val_dataloader = DataLoader(val_examples, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_v2_lr1.pth\")\n",
    "lrs={\"lr1\":0.00001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "else:\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 360/360 [01:00<00:00,  5.92it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:54<00:00,  6.65it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.68it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.80it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:54<00:00,  6.62it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.74it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:54<00:00,  6.66it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:54<00:00,  6.62it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.70it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.81it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.70it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:54<00:00,  6.59it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.73it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.81it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.85it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.70it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.78it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.67it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:55<00:00,  6.46it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.81it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.69it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.76it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.77it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.80it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.83it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.73it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.86it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.68it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:54<00:00,  6.61it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.82it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.75it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.81it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:51<00:00,  6.95it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.81it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.72it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.82it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:51<00:00,  6.92it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.82it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:55<00:00,  6.52it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.77it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.88it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.90it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.68it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.72it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.73it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.74it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.76it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:00<00:00,  5.94it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:19<00:00,  4.55it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:19<00:00,  4.51it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:13<00:00,  4.88it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:14<00:00,  4.86it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:16<00:00,  4.68it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:16<00:00,  4.68it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:13<00:00,  4.87it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:14<00:00,  4.80it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:15<00:00,  4.79it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:16<00:00,  4.73it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:14<00:00,  4.85it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:14<00:00,  4.82it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:17<00:00,  4.67it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:11<00:00,  5.00it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:14<00:00,  4.83it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:10<00:00,  5.07it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:55<00:00,  6.46it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:52<00:00,  6.83it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.75it/s]\n",
      "Iteration: 100%|██████████| 360/360 [00:53<00:00,  6.78it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:00<00:00,  5.90it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:13<00:00,  4.93it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:12<00:00,  4.97it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:14<00:00,  4.86it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:16<00:00,  4.69it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:14<00:00,  4.82it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:14<00:00,  4.80it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:12<00:00,  4.97it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:14<00:00,  4.84it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:12<00:00,  4.94it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:12<00:00,  4.99it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:16<00:00,  4.71it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:12<00:00,  4.97it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:12<00:00,  4.95it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:13<00:00,  4.90it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:11<00:00,  5.07it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:15<00:00,  4.75it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:14<00:00,  4.81it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:19<00:00,  4.54it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:18<00:00,  4.56it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:22<00:00,  4.36it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:12<00:00,  4.94it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:09<00:00,  5.14it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:08<00:00,  5.22it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:08<00:00,  5.25it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:08<00:00,  5.29it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:07<00:00,  5.34it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:07<00:00,  5.32it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:07<00:00,  5.30it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:06<00:00,  5.45it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:06<00:00,  5.40it/s]\n",
      "Iteration: 100%|██████████| 360/360 [01:08<00:00,  5.26it/s]\n",
      "Epoch: 100%|██████████| 100/100 [1:50:35<00:00, 66.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "\n",
    "model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "sentences1 = []\n",
    "sentences2 = []\n",
    "scores = []\n",
    "for i,row in validation_cosine_labels.iterrows():\n",
    "    sentences1.append(row[\"sentence1\"])\n",
    "    sentences2.append(row[\"sentence2\"])\n",
    "    scores.append(row[\"label\"])\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=100,\n",
    "    warmup_steps=100,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_senbert.pth\")\n",
    "lrs={\"lr1\":0.00001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(\"stsb_test.csv\")\n",
    "df=df.drop(\"idx\",axis=1)\n",
    "test_cosine_labels = df.copy() \n",
    "test_cosine_labels[\"label\"] = test_cosine_labels[\"label\"] /test_cosine_labels[\"label\"].abs().max() \n",
    "test_examples = []\n",
    "for i,row in test_cosine_labels.iterrows():\n",
    "    test_examples.append(InputExample(texts=[row[\"sentence1\"], row[\"sentence2\"]],label=row[\"label\"]))\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(test_examples, shuffle=True, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person who cobbles . \t\t Nonsense . \t\t Score: 0.4383\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
    "model.load_state_dict(torch.load(\"model_senbert.pth\"))\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = [\n",
    "\"A person who cobbles .\",\n",
    "]\n",
    "\n",
    "sentences2 = [\n",
    "\"Nonsense .\",\n",
    "]\n",
    "\n",
    "# Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "# Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(\n",
    "        sentences1[i], sentences2[i], cosine_scores[i][i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_points = np.random.rand(3387, 7929)\n",
    "\n",
    "# Assuming 'centroid' is your centroid array of shape (1, 7929)\n",
    "centroid = np.random.rand(1, 7929)\n",
    "\n",
    "# Compute the Euclidean distance\n",
    "distances = np.sqrt(np.sum((data_points - centroid) ** 2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20668231, 0.45125576, 0.40175181, ..., 0.63991649, 0.21395359,\n",
       "        0.88787819],\n",
       "       [0.30406374, 0.16359059, 0.64448195, ..., 0.33988998, 0.19543703,\n",
       "        0.906416  ],\n",
       "       [0.69443658, 0.4957568 , 0.32649517, ..., 0.86524676, 0.25989218,\n",
       "        0.76663841],\n",
       "       ...,\n",
       "       [0.74186748, 0.43780957, 0.23715375, ..., 0.45474528, 0.91249342,\n",
       "        0.23555313],\n",
       "       [0.01294029, 0.34696805, 0.99877482, ..., 0.99478881, 0.6308167 ,\n",
       "        0.45844663],\n",
       "       [0.40135945, 0.08331242, 0.57972066, ..., 0.33972442, 0.1514921 ,\n",
       "        0.70476375]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7929)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdistances\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "distances[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
